{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced autotune tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DISCLAIMER: Most experiments in this notebook require one or more GPUs to keep their runtime a matter of hours.**\n",
    "**DISCLAIMER: To use our new autotune feature in parallel mode, you need to install [MongoDb](https://docs.mongodb.com/manual/installation/) first.**\n",
    "\n",
    "In this notebook, we give an in-depth tutorial on `scVI`'s new `autotune` module.\n",
    "\n",
    "Overall, the new module enables users to perform parallel hyperparemter search for any scVI model and on any number of GPUs/CPUs. Although, the search may be performed sequentially using only one GPU/CPU, we will focus on the paralel case.\n",
    "Note that GPUs provide a much faster approach as they are particularly suitable for neural networks gradient back-propagation.\n",
    "\n",
    "Additionally, we provide the code used to generate the results presented in our [Hyperoptimization blog post](https://yoseflab.github.io/2019/07/05/Hyperoptimization/). For an in-depth analysis of the results obtained on three gold standard scRNAseq datasets (Cortex, PBMC and BrainLarge), please to the above blog post. In the blog post, we also suggest guidelines on how and when to use our auto-tuning feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rich'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-28d72a8e5914>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhyperopt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscvi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscvi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcortex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpbmc_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrainlarge_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotation_simulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscvi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauto_tune_scvi_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scvi_dev/scvi/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_constants\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_CONSTANTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_settings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scvi_dev/scvi/_settings.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_compat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrich\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRichHandler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrich\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsole\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConsole\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rich'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import scanpy\n",
    "import anndata\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from hyperopt import hp\n",
    "\n",
    "import scvi\n",
    "from scvi.data import cortex, pbmc_dataset, brainlarge_dataset, annotation_simulation\n",
    "from scvi.inference import auto_tune_scvi_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"scvi.inference.autotune\")\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allow_notebook_for_test():\n",
    "    print(\"Testing the autotune advanced notebook\")\n",
    "\n",
    "test_mode = False\n",
    "\n",
    "\n",
    "def if_not_test_else(x, y):\n",
    "    if not test_mode:\n",
    "        return x\n",
    "    else:\n",
    "        return y\n",
    "\n",
    "\n",
    "save_path = \"data/\"\n",
    "n_epochs = if_not_test_else(1000, 1)\n",
    "n_epochs_brain_large = if_not_test_else(50, 1)\n",
    "max_evals = if_not_test_else(100, 1)\n",
    "reserve_timeout = if_not_test_else(180, 5)\n",
    "fmin_timeout = if_not_test_else(300, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of principled simplicity, we provide an all-default approach to hyperparameter search for any `scVI` model.\n",
    "The few lines below present an example of how to perform hyper-parameter search for `scVI` on the Cortex dataset.\n",
    "\n",
    "Note that, by default, the model used is `scVI`'s `VAE` and the trainer is the `UnsupervisedTrainer`\n",
    "\n",
    "Also, the default search space is as follows:\n",
    "\n",
    "* `n_latent`: [5, 15]\n",
    "* `n_hidden`: {64, 128, 256}\n",
    "* `n_layers`: [1, 5]\n",
    "* `dropout_rate`: {0.1, 0.3, 0.5, 0.7}\n",
    "* `reconstruction_loss`: {\"zinb\", \"nb\"}\n",
    "* `lr`: {0.01, 0.005, 0.001, 0.0005, 0.0001}\n",
    "\n",
    "On a more practical note, verbosity varies in the following way:\n",
    "\n",
    "* `logger.setLevel(logging.WARNING)` will show a progress bar.\n",
    "* `logger.setLevel(logging.INFO)` will show global logs including the number of jobs done.\n",
    "* `logger.setLevel(logging.DEBUG)` will show detailed logs for each training (e.g the parameters tested).\n",
    "\n",
    "This function's behaviour can be customized, please refer to the rest of this tutorial as well as its documentation for information about the different parameters available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the hyperoptimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scvi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a15587a23fcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcortex_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscvi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcortex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'scvi' is not defined"
     ]
    }
   ],
   "source": [
    "cortex_dataset = scvi.data.cortex(save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-07-17 21:16:45,659] INFO - scvi.inference.autotune.all | Starting experiment: cortex_dataset\n",
      "[2020-07-17 21:16:45,660] DEBUG - scvi.inference.autotune.all | Using default parameter search space.\n",
      "[2020-07-17 21:16:45,662] DEBUG - scvi.inference.autotune.all | Adding default early stopping behaviour.\n",
      "[2020-07-17 21:16:45,663] INFO - scvi.inference.autotune.all | Fixed parameters: \n",
      "model: \n",
      "{}\n",
      "trainer: \n",
      "{'early_stopping_kwargs': {'early_stopping_metric': 'elbo', 'save_best_state_metric': 'elbo', 'patience': 50, 'threshold': 0, 'reduce_lr_on_plateau': True, 'lr_patience': 25, 'lr_factor': 0.2}, 'metrics_to_monitor': ['elbo']}\n",
      "train method: \n",
      "{'n_epochs': 1}\n",
      "[2020-07-17 21:16:45,663] INFO - scvi.inference.autotune.all | Starting parallel hyperoptimization\n",
      "[2020-07-17 21:16:45,671] DEBUG - scvi.inference.autotune.all | Starting MongoDb process, logs redirected to ./mongo/mongo_logfile.txt.\n",
      "[2020-07-17 21:16:50,697] DEBUG - scvi.inference.autotune.all | Starting minimization procedure\n",
      "[2020-07-17 21:16:50,702] DEBUG - scvi.inference.autotune.all | Starting FminProcess.\n",
      "[2020-07-17 21:16:50,702] DEBUG - scvi.inference.autotune.all | Starting worker launcher\n",
      "[2020-07-17 21:16:50,705] DEBUG - scvi.inference.autotune.all | gpu_ids is None, defaulting to all 0 GPUs found by torch.\n",
      "[2020-07-17 21:16:50,708] DEBUG - scvi.inference.autotune.all | No GPUs found and n_cpu_wokers is None, defaulting to n_cpu_workers = 7 (os.cpu_count() - 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-07-17 21:16:50,712] DEBUG - scvi.inference.autotune.all | Listener listening...\n",
      "[2020-07-17 21:16:50,712] INFO - scvi.inference.autotune.all | Starting 1 worker.s for each of the 0 gpu.s set for use/found.\n",
      "[2020-07-17 21:16:50,714] INFO - scvi.inference.autotune.all | Starting 7 cpu worker.s\n",
      "[2020-07-17 21:16:50,723] DEBUG - scvi.inference.autotune.all | No timer, waiting for fmin...\n",
      "[2020-07-17 21:17:05,506] DEBUG - scvi.inference.autotune.all | All workers have died, check stdout/stderr for error tracebacks.\n",
      "[2020-07-17 21:17:05,507] DEBUG - scvi.inference.autotune.all | Worker watchdog finished, terminating workers and stopping listener.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-07-17 21:17:10,719] DEBUG - scvi.inference.autotune.all | multiple_hosts set to false, Fmin has 10 seconds to finish\n",
      "[2020-07-17 21:17:20,725] ERROR - scvi.inference.autotune.all | Queue still empty 10 seconds after all workers have died.\n",
      "Terminating minimization process.\n",
      "[2020-07-17 21:17:20,726] ERROR - scvi.inference.autotune.all | Caught ('Queue still empty 10 seconds after all workers have died. Check that you have used a new exp_key or allowed a higher max_evals',) in auto_tune_scvi_model, starting cleanup.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/galen/scVI/scvi/inference/autotune.py\", line 725, in _auto_tune_parallel\n",
      "    trials = queue.get(timeout=fmin_timeout)\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 105, in get\n",
      "    raise Empty\n",
      "_queue.Empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/galen/scVI/scvi/inference/autotune.py\", line 146, in decorated\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/galen/scVI/scvi/inference/autotune.py\", line 470, in auto_tune_scvi_model\n",
      "    multiple_hosts=multiple_hosts,\n",
      "  File \"/Users/galen/scVI/scvi/inference/autotune.py\", line 735, in _auto_tune_parallel\n",
      "    \"a higher max_evals\".format(fmin_timeout=fmin_timeout)\n",
      "scvi.inference.autotune.FminTimeoutError: Queue still empty 10 seconds after all workers have died. Check that you have used a new exp_key or allowed a higher max_evals\n",
      "[2020-07-17 21:17:20,730] INFO - scvi.inference.autotune.all | Cleaning up\n",
      "[2020-07-17 21:17:20,731] DEBUG - scvi.inference.autotune.all | Cleaning up: closing files.\n",
      "[2020-07-17 21:17:20,732] DEBUG - scvi.inference.autotune.all | Cleaning up: closing queues.\n",
      "[2020-07-17 21:17:20,732] DEBUG - scvi.inference.autotune.all | Cleaning up: setting cleanup_event and joining threads.\n",
      "[2020-07-17 21:17:20,733] DEBUG - scvi.inference.autotune.all | Thread Progress Listener already done.\n",
      "[2020-07-17 21:17:20,734] DEBUG - scvi.inference.autotune.all | Thread Worker Launcher already done.\n",
      "[2020-07-17 21:17:20,735] DEBUG - scvi.inference.autotune.all | Closing Thread Fmin Launcher.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/threading.py\", line 865, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/logging/handlers.py\", line 1475, in _monitor\n",
      "    record = self.dequeue(True)\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/logging/handlers.py\", line 1424, in dequeue\n",
      "    return self.queue.get(block)\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 94, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 383, in _recv\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-07-17 21:17:30,737] DEBUG - scvi.inference.autotune.all | fmin finished.\n",
      "[2020-07-17 21:17:30,739] DEBUG - scvi.inference.autotune.all | Cleaning up: terminating processes.\n",
      "[2020-07-17 21:17:30,741] DEBUG - scvi.inference.autotune.all | Process Worker CPU 6 already done.\n",
      "[2020-07-17 21:17:30,743] DEBUG - scvi.inference.autotune.all | Process Worker CPU 5 already done.\n",
      "[2020-07-17 21:17:30,743] DEBUG - scvi.inference.autotune.all | Process Worker CPU 4 already done.\n",
      "[2020-07-17 21:17:30,745] DEBUG - scvi.inference.autotune.all | Process Worker CPU 3 already done.\n",
      "[2020-07-17 21:17:30,746] DEBUG - scvi.inference.autotune.all | Process Worker CPU 2 already done.\n",
      "[2020-07-17 21:17:30,747] DEBUG - scvi.inference.autotune.all | Process Worker CPU 1 already done.\n",
      "[2020-07-17 21:17:30,747] DEBUG - scvi.inference.autotune.all | Process Worker CPU 0 already done.\n",
      "[2020-07-17 21:17:30,748] DEBUG - scvi.inference.autotune.all | Terminating Process Fmin.\n",
      "[2020-07-17 21:17:30,749] DEBUG - scvi.inference.autotune.all | Terminating mongod process.\n",
      "[2020-07-17 21:17:30,750] DEBUG - scvi.inference.autotune.all | Cleaning up: removing added logging handler.\n",
      "[2020-07-17 21:17:30,751] DEBUG - scvi.inference.autotune.all | Cleaning up: removing hyperopt FileHandler.\n",
      "[2020-07-17 21:17:30,752] DEBUG - scvi.inference.autotune.all | Cleaning up: removing autotune FileHandler.\n"
     ]
    },
    {
     "ename": "FminTimeoutError",
     "evalue": "Queue still empty 10 seconds after all workers have died. Check that you have used a new exp_key or allowed a higher max_evals",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m~/scVI/scvi/inference/autotune.py\u001b[0m in \u001b[0;36m_auto_tune_parallel\u001b[0;34m(objective_hyperopt, exp_key, space, max_evals, save_path, n_cpu_workers, gpu_ids, n_workers_per_gpu, reserve_timeout, fmin_timeout, fmin_timer, mongo_port, mongo_host, db_name, multiple_hosts)\u001b[0m\n\u001b[1;32m    724\u001b[0m             )\n\u001b[0;32m--> 725\u001b[0;31m         \u001b[0mtrials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfmin_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m         \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFminTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-eb9feb949bd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mreserve_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreserve_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mfmin_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfmin_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;32m~/scVI/scvi/inference/autotune.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             logger_all.exception(\n",
      "\u001b[0;32m~/scVI/scvi/inference/autotune.py\u001b[0m in \u001b[0;36mauto_tune_scvi_model\u001b[0;34m(exp_key, gene_dataset, delayed_populating, custom_objective_hyperopt, objective_kwargs, model_class, trainer_class, metric_name, metric_kwargs, posterior_name, model_specific_kwargs, trainer_specific_kwargs, train_func_specific_kwargs, space, max_evals, train_best, pickle_result, save_path, use_batches, parallel, n_cpu_workers, gpu_ids, n_workers_per_gpu, reserve_timeout, fmin_timeout, fmin_timer, mongo_port, mongo_host, db_name, multiple_hosts)\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0mmongo_host\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmongo_host\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0mdb_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdb_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m             \u001b[0mmultiple_hosts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmultiple_hosts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m         )\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scVI/scvi/inference/autotune.py\u001b[0m in \u001b[0;36m_auto_tune_parallel\u001b[0;34m(objective_hyperopt, exp_key, space, max_evals, save_path, n_cpu_workers, gpu_ids, n_workers_per_gpu, reserve_timeout, fmin_timeout, fmin_timer, mongo_port, mongo_host, db_name, multiple_hosts)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;34m\"Queue still empty {fmin_timeout} seconds after all workers \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0;34m\"have died. Check that you have used a new exp_key or allowed \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m             \u001b[0;34m\"a higher max_evals\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfmin_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfmin_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m         )\n\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFminTimeoutError\u001b[0m: Queue still empty 10 seconds after all workers have died. Check that you have used a new exp_key or allowed a higher max_evals"
     ]
    }
   ],
   "source": [
    "best_vae, trials = auto_tune_scvi_model(\n",
    "    gene_dataset=cortex_dataset,\n",
    "    parallel=True,\n",
    "    exp_key=\"cortex_dataset\",\n",
    "    train_func_specific_kwargs={\"n_epochs\": n_epochs},\n",
    "    max_evals=max_evals,\n",
    "    reserve_timeout=reserve_timeout,\n",
    "    fmin_timeout=fmin_timeout,\n",
    ")\n",
    "latent = best_vae.get_latent_representation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Returned objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `trials` object contains detailed information about each run.\n",
    "`trials.trials` is an `Iterable` in which each element corresponds to a single run. It can be used as a dictionary for wich the key \"result\" yields a dictionnary containing the outcome of the run as defined in our default objective function (or the user's custom version). For example, it will contain information on the hyperparameters used (under the \"space\" key), the resulting metric (under the \"loss\" key) or the status of the run.\n",
    "\n",
    "The `best_trainer` object can be used directly as an scVI `Trainer` object. It is the result of a training on the whole dataset provided using the optimal set of hyperparameters found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom hyperamater space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although our default can be a good one in a number of cases, we still provide an easy way to use custom values for the hyperparameters search space.\n",
    "These are broken down in three categories:\n",
    "\n",
    "* Hyperparameters for the `Trainer` instance. (if any)\n",
    "* Hyperparameters for the `Trainer` instance's `train` method. (e.g `lr`)\n",
    "* Hyperparameters for the model instance. (e.g `n_layers`)\n",
    "\n",
    "To build your own hyperparameter space follow the scheme used in `scVI`'s codebase as well as the sample below.\n",
    "Note the various spaces you define, have to follow the `hyperopt` syntax, for which you can find a detailed description [here](https://github.com/hyperopt/hyperopt/wiki/FMin#2-defining-a-search-space).\n",
    "\n",
    "For example, if you were to want to search over a continuous range of droupouts varying in [0.1, 0.3] and for a continuous learning rate varying in [0.001, 0.0001], you could use the following search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-07-17 17:38:46,936] INFO - scvi.inference.autotune.all | Starting experiment: cortex_dataset_custom_space\n",
      "[2020-07-17 17:38:46,937] DEBUG - scvi.inference.autotune.all | Adding default early stopping behaviour.\n",
      "[2020-07-17 17:38:46,939] INFO - scvi.inference.autotune.all | Fixed parameters: \n",
      "model: \n",
      "{}\n",
      "trainer: \n",
      "{'early_stopping_kwargs': {'early_stopping_metric': 'elbo', 'save_best_state_metric': 'elbo', 'patience': 50, 'threshold': 0, 'reduce_lr_on_plateau': True, 'lr_patience': 25, 'lr_factor': 0.2}, 'metrics_to_monitor': ['elbo']}\n",
      "train method: \n",
      "{'n_epochs': 1}\n",
      "[2020-07-17 17:38:46,940] INFO - scvi.inference.autotune.all | Starting parallel hyperoptimization\n",
      "[2020-07-17 17:38:46,942] DEBUG - scvi.inference.autotune.all | Starting MongoDb process, logs redirected to ./mongo/mongo_logfile.txt.\n",
      "[2020-07-17 17:38:51,973] DEBUG - scvi.inference.autotune.all | Starting minimization procedure\n",
      "[2020-07-17 17:38:51,975] DEBUG - scvi.inference.autotune.all | Starting FminProcess.\n",
      "[2020-07-17 17:38:51,975] DEBUG - scvi.inference.autotune.all | Starting worker launcher\n",
      "[2020-07-17 17:38:51,978] DEBUG - scvi.inference.autotune.all | gpu_ids is None, defaulting to all 0 GPUs found by torch.\n",
      "[2020-07-17 17:38:52,046] DEBUG - scvi.inference.autotune.all | No GPUs found and n_cpu_wokers is None, defaulting to n_cpu_workers = 7 (os.cpu_count() - 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-07-17 17:38:52,067] DEBUG - scvi.inference.autotune.all | Listener listening...\n",
      "[2020-07-17 17:38:52,068] INFO - scvi.inference.autotune.all | Starting 1 worker.s for each of the 0 gpu.s set for use/found.\n",
      "[2020-07-17 17:38:52,072] INFO - scvi.inference.autotune.all | Starting 7 cpu worker.s\n",
      "[2020-07-17 17:39:00,504] DEBUG - scvi.inference.autotune.all | No timer, waiting for fmin...\n",
      "[2020-07-17 17:39:06,405] DEBUG - scvi.inference.autotune.all | All workers have died, check stdout/stderr for error tracebacks.\n",
      "[2020-07-17 17:39:06,406] DEBUG - scvi.inference.autotune.all | Worker watchdog finished, terminating workers and stopping listener.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-07-17 17:39:10,512] DEBUG - scvi.inference.autotune.all | fmin finished.\n",
      "[2020-07-17 17:39:11,984] DEBUG - scvi.inference.autotune.all | Setting worker launcher stop event.\n",
      "[2020-07-17 17:39:11,986] DEBUG - scvi.inference.autotune.all | multiple_hosts set to false, Fmin has 10 seconds to finish\n",
      "[2020-07-17 17:39:21,994] ERROR - scvi.inference.autotune.all | Queue still empty 10 seconds after all workers have died.\n",
      "Terminating minimization process.\n",
      "[2020-07-17 17:39:21,995] ERROR - scvi.inference.autotune.all | Caught ('Queue still empty 10 seconds after all workers have died. Check that you have used a new exp_key or allowed a higher max_evals',) in auto_tune_scvi_model, starting cleanup.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/galen/scVI/scvi/inference/autotune.py\", line 725, in _auto_tune_parallel\n",
      "    trials = queue.get(timeout=fmin_timeout)\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 105, in get\n",
      "    raise Empty\n",
      "_queue.Empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/galen/scVI/scvi/inference/autotune.py\", line 146, in decorated\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/galen/scVI/scvi/inference/autotune.py\", line 470, in auto_tune_scvi_model\n",
      "    multiple_hosts=multiple_hosts,\n",
      "  File \"/Users/galen/scVI/scvi/inference/autotune.py\", line 735, in _auto_tune_parallel\n",
      "    \"a higher max_evals\".format(fmin_timeout=fmin_timeout)\n",
      "scvi.inference.autotune.FminTimeoutError: Queue still empty 10 seconds after all workers have died. Check that you have used a new exp_key or allowed a higher max_evals\n",
      "[2020-07-17 17:39:21,997] INFO - scvi.inference.autotune.all | Cleaning up\n",
      "[2020-07-17 17:39:21,998] DEBUG - scvi.inference.autotune.all | Cleaning up: closing files.\n",
      "[2020-07-17 17:39:22,000] DEBUG - scvi.inference.autotune.all | Cleaning up: closing queues.\n",
      "[2020-07-17 17:39:22,001] DEBUG - scvi.inference.autotune.all | Cleaning up: setting cleanup_event and joining threads.\n",
      "[2020-07-17 17:39:22,002] DEBUG - scvi.inference.autotune.all | Thread Progress Listener already done.\n",
      "[2020-07-17 17:39:22,003] DEBUG - scvi.inference.autotune.all | Thread Worker Launcher already done.\n",
      "[2020-07-17 17:39:22,004] DEBUG - scvi.inference.autotune.all | Thread Fmin Launcher already done.\n",
      "[2020-07-17 17:39:22,007] DEBUG - scvi.inference.autotune.all | Thread Progress Listener already done.\n",
      "[2020-07-17 17:39:22,008] DEBUG - scvi.inference.autotune.all | Thread Worker Launcher already done.\n",
      "[2020-07-17 17:39:22,009] DEBUG - scvi.inference.autotune.all | Thread Fmin Launcher already done.\n",
      "[2020-07-17 17:39:22,011] DEBUG - scvi.inference.autotune.all | Thread Progress Listener already done.\n",
      "[2020-07-17 17:39:22,012] DEBUG - scvi.inference.autotune.all | Thread Worker Launcher already done.\n",
      "[2020-07-17 17:39:22,013] DEBUG - scvi.inference.autotune.all | Thread Fmin Launcher already done.\n",
      "[2020-07-17 17:39:22,014] DEBUG - scvi.inference.autotune.all | Thread Progress Listener already done.\n",
      "[2020-07-17 17:39:22,015] DEBUG - scvi.inference.autotune.all | Thread Worker Launcher already done.\n",
      "[2020-07-17 17:39:22,016] DEBUG - scvi.inference.autotune.all | Thread Fmin Launcher already done.\n",
      "[2020-07-17 17:39:22,016] DEBUG - scvi.inference.autotune.all | Thread Progress Listener already done.\n",
      "[2020-07-17 17:39:22,022] DEBUG - scvi.inference.autotune.all | Thread Worker Launcher already done.\n",
      "[2020-07-17 17:39:22,028] DEBUG - scvi.inference.autotune.all | Thread Fmin Launcher already done.\n",
      "[2020-07-17 17:39:22,032] DEBUG - scvi.inference.autotune.all | Cleaning up: terminating processes.\n",
      "[2020-07-17 17:39:22,034] DEBUG - scvi.inference.autotune.all | Process Fmin already done.\n",
      "[2020-07-17 17:39:22,034] DEBUG - scvi.inference.autotune.all | Process Worker CPU 6 already done.\n",
      "[2020-07-17 17:39:22,036] DEBUG - scvi.inference.autotune.all | Process Worker CPU 5 already done.\n",
      "[2020-07-17 17:39:22,037] DEBUG - scvi.inference.autotune.all | Process Worker CPU 4 already done.\n",
      "[2020-07-17 17:39:22,043] DEBUG - scvi.inference.autotune.all | Process Worker CPU 3 already done.\n",
      "[2020-07-17 17:39:22,047] DEBUG - scvi.inference.autotune.all | Process Worker CPU 2 already done.\n",
      "[2020-07-17 17:39:22,053] DEBUG - scvi.inference.autotune.all | Process Worker CPU 1 already done.\n",
      "[2020-07-17 17:39:22,054] DEBUG - scvi.inference.autotune.all | Process Worker CPU 0 already done.\n",
      "[2020-07-17 17:39:22,055] DEBUG - scvi.inference.autotune.all | Terminating mongod process.\n",
      "[2020-07-17 17:39:22,057] DEBUG - scvi.inference.autotune.all | Process Fmin already done.\n",
      "[2020-07-17 17:39:22,058] DEBUG - scvi.inference.autotune.all | Process Worker CPU 6 already done.\n",
      "[2020-07-17 17:39:22,059] DEBUG - scvi.inference.autotune.all | Process Worker CPU 5 already done.\n",
      "[2020-07-17 17:39:22,060] DEBUG - scvi.inference.autotune.all | Process Worker CPU 4 already done.\n",
      "[2020-07-17 17:39:22,061] DEBUG - scvi.inference.autotune.all | Process Worker CPU 3 already done.\n",
      "[2020-07-17 17:39:22,062] DEBUG - scvi.inference.autotune.all | Process Worker CPU 2 already done.\n",
      "[2020-07-17 17:39:22,063] DEBUG - scvi.inference.autotune.all | Process Worker CPU 1 already done.\n",
      "[2020-07-17 17:39:22,063] DEBUG - scvi.inference.autotune.all | Process Worker CPU 0 already done.\n",
      "[2020-07-17 17:39:22,064] DEBUG - scvi.inference.autotune.all | Terminating mongod process.\n",
      "[2020-07-17 17:39:22,065] DEBUG - scvi.inference.autotune.all | Process Fmin already done.\n",
      "[2020-07-17 17:39:22,066] DEBUG - scvi.inference.autotune.all | Process Worker CPU 6 already done.\n",
      "[2020-07-17 17:39:22,066] DEBUG - scvi.inference.autotune.all | Process Worker CPU 5 already done.\n",
      "[2020-07-17 17:39:22,067] DEBUG - scvi.inference.autotune.all | Process Worker CPU 4 already done.\n",
      "[2020-07-17 17:39:22,068] DEBUG - scvi.inference.autotune.all | Process Worker CPU 3 already done.\n",
      "[2020-07-17 17:39:22,068] DEBUG - scvi.inference.autotune.all | Process Worker CPU 2 already done.\n",
      "[2020-07-17 17:39:22,069] DEBUG - scvi.inference.autotune.all | Process Worker CPU 1 already done.\n",
      "[2020-07-17 17:39:22,069] DEBUG - scvi.inference.autotune.all | Process Worker CPU 0 already done.\n",
      "[2020-07-17 17:39:22,070] DEBUG - scvi.inference.autotune.all | Terminating mongod process.\n",
      "[2020-07-17 17:39:22,071] DEBUG - scvi.inference.autotune.all | Process Fmin already done.\n",
      "[2020-07-17 17:39:22,072] DEBUG - scvi.inference.autotune.all | Process Worker CPU 6 already done.\n",
      "[2020-07-17 17:39:22,072] DEBUG - scvi.inference.autotune.all | Process Worker CPU 5 already done.\n",
      "[2020-07-17 17:39:22,073] DEBUG - scvi.inference.autotune.all | Process Worker CPU 4 already done.\n",
      "[2020-07-17 17:39:22,073] DEBUG - scvi.inference.autotune.all | Process Worker CPU 3 already done.\n",
      "[2020-07-17 17:39:22,074] DEBUG - scvi.inference.autotune.all | Process Worker CPU 2 already done.\n",
      "[2020-07-17 17:39:22,075] DEBUG - scvi.inference.autotune.all | Process Worker CPU 1 already done.\n",
      "[2020-07-17 17:39:22,075] DEBUG - scvi.inference.autotune.all | Process Worker CPU 0 already done.\n",
      "[2020-07-17 17:39:22,076] DEBUG - scvi.inference.autotune.all | Terminating mongod process.\n",
      "[2020-07-17 17:39:22,077] DEBUG - scvi.inference.autotune.all | Process Fmin already done.\n",
      "[2020-07-17 17:39:22,078] DEBUG - scvi.inference.autotune.all | Process Worker CPU 6 already done.\n",
      "[2020-07-17 17:39:22,079] DEBUG - scvi.inference.autotune.all | Process Worker CPU 5 already done.\n",
      "[2020-07-17 17:39:22,080] DEBUG - scvi.inference.autotune.all | Process Worker CPU 4 already done.\n",
      "[2020-07-17 17:39:22,080] DEBUG - scvi.inference.autotune.all | Process Worker CPU 3 already done.\n",
      "[2020-07-17 17:39:22,081] DEBUG - scvi.inference.autotune.all | Process Worker CPU 2 already done.\n",
      "[2020-07-17 17:39:22,081] DEBUG - scvi.inference.autotune.all | Process Worker CPU 1 already done.\n",
      "[2020-07-17 17:39:22,082] DEBUG - scvi.inference.autotune.all | Process Worker CPU 0 already done.\n",
      "[2020-07-17 17:39:22,083] DEBUG - scvi.inference.autotune.all | Terminating mongod process.\n",
      "[2020-07-17 17:39:22,084] DEBUG - scvi.inference.autotune.all | Cleaning up: removing added logging handler.\n",
      "[2020-07-17 17:39:22,085] DEBUG - scvi.inference.autotune.all | Cleaning up: removing hyperopt FileHandler.\n",
      "[2020-07-17 17:39:22,086] DEBUG - scvi.inference.autotune.all | Cleaning up: removing autotune FileHandler.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-13:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/threading.py\", line 865, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/logging/handlers.py\", line 1475, in _monitor\n",
      "    record = self.dequeue(True)\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/logging/handlers.py\", line 1424, in dequeue\n",
      "    return self.queue.get(block)\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 94, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/galen/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 383, in _recv\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n"
     ]
    },
    {
     "ename": "FminTimeoutError",
     "evalue": "Queue still empty 10 seconds after all workers have died. Check that you have used a new exp_key or allowed a higher max_evals",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m~/scVI/scvi/inference/autotune.py\u001b[0m in \u001b[0;36m_auto_tune_parallel\u001b[0;34m(objective_hyperopt, exp_key, space, max_evals, save_path, n_cpu_workers, gpu_ids, n_workers_per_gpu, reserve_timeout, fmin_timeout, fmin_timer, mongo_port, mongo_host, db_name, multiple_hosts)\u001b[0m\n\u001b[1;32m    724\u001b[0m             )\n\u001b[0;32m--> 725\u001b[0;31m         \u001b[0mtrials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfmin_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m         \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFminTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-167205b30336>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mreserve_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreserve_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mfmin_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfmin_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m )\n",
      "\u001b[0;32m~/scVI/scvi/inference/autotune.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             logger_all.exception(\n",
      "\u001b[0;32m~/scVI/scvi/inference/autotune.py\u001b[0m in \u001b[0;36mauto_tune_scvi_model\u001b[0;34m(exp_key, gene_dataset, delayed_populating, custom_objective_hyperopt, objective_kwargs, model_class, trainer_class, metric_name, metric_kwargs, posterior_name, model_specific_kwargs, trainer_specific_kwargs, train_func_specific_kwargs, space, max_evals, train_best, pickle_result, save_path, use_batches, parallel, n_cpu_workers, gpu_ids, n_workers_per_gpu, reserve_timeout, fmin_timeout, fmin_timer, mongo_port, mongo_host, db_name, multiple_hosts)\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0mmongo_host\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmongo_host\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0mdb_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdb_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m             \u001b[0mmultiple_hosts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmultiple_hosts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m         )\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scVI/scvi/inference/autotune.py\u001b[0m in \u001b[0;36m_auto_tune_parallel\u001b[0;34m(objective_hyperopt, exp_key, space, max_evals, save_path, n_cpu_workers, gpu_ids, n_workers_per_gpu, reserve_timeout, fmin_timeout, fmin_timer, mongo_port, mongo_host, db_name, multiple_hosts)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;34m\"Queue still empty {fmin_timeout} seconds after all workers \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0;34m\"have died. Check that you have used a new exp_key or allowed \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m             \u001b[0;34m\"a higher max_evals\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfmin_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfmin_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m         )\n\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFminTimeoutError\u001b[0m: Queue still empty 10 seconds after all workers have died. Check that you have used a new exp_key or allowed a higher max_evals"
     ]
    }
   ],
   "source": [
    "space = {\n",
    "    \"model_tunable_kwargs\": {\"dropout_rate\": hp.uniform(\"dropout_rate\", 0.1, 0.3)},\n",
    "    \"train_func_tunable_kwargs\": {\"lr\": hp.loguniform(\"lr\", -4.0, -3.0)},\n",
    "}\n",
    "\n",
    "best_vae, trials = auto_tune_scvi_model(\n",
    "    gene_dataset=cortex_dataset,\n",
    "    space=space,\n",
    "    parallel=True,\n",
    "    exp_key=\"cortex_dataset_custom_space\",\n",
    "    train_func_specific_kwargs={\"n_epochs\": n_epochs},\n",
    "    max_evals=max_evals,\n",
    "    reserve_timeout=reserve_timeout,\n",
    "    fmin_timeout=fmin_timeout,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom objective metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, our autotune process tracks the marginal negative log likelihood of the best state of the model according ot the held-out Evidence Lower BOund (ELBO). But, if you want to track a different early stopping metric and optimize a different loss you can use `auto_tune_scvi_model`'s parameters.\n",
    "\n",
    "For example, if for some reason, you had a dataset coming from two batches (i.e two merged datasets) and wanted to optimize the hyperparameters for the batch mixing entropy. You could use the code below, which makes use of the `metric_name` argument of `auto_tune_scvi_model`. This can work for any metric that is implemented in the  `AnnDataLoader` class you use. You may also specify the name of the `AnnDataLoader` attribute you want to use (e.g \"train_set\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-07-17 21:12:09,530] INFO - scvi.dataset._utils | Downloading file at data/10X/gene_info_pbmc.csv\n",
      "[2020-07-17 21:12:10,461] INFO - scvi.dataset._utils | Downloading file at data/10X/pbmc_metadata.pickle\n",
      "[2020-07-17 21:12:11,733] INFO - scvi.dataset._utils | Downloading file at data/10X/pbmc8k/filtered_gene_bc_matrices.tar.gz\n",
      "[2020-07-17 21:12:13,217] INFO - scvi.dataset.dataset10X | Extracting tar file\n",
      "data/10X/pbmc8k/filtered_gene_bc_matrices/GRCh38\n",
      "/Users/galen/scVI/tests/notebooks\n",
      "[2020-07-17 21:12:30,831] INFO - scvi.dataset.dataset10X | Removing extracted data at data/10X/pbmc8k/filtered_gene_bc_matrices\n",
      "[2020-07-17 21:12:31,862] INFO - scvi.dataset._utils | Downloading file at data/10X/pbmc4k/filtered_gene_bc_matrices.tar.gz\n",
      "[2020-07-17 21:12:33,073] INFO - scvi.dataset.dataset10X | Extracting tar file\n",
      "data/10X/pbmc4k/filtered_gene_bc_matrices/GRCh38\n",
      "/Users/galen/scVI/tests/notebooks\n",
      "[2020-07-17 21:12:41,448] INFO - scvi.dataset.dataset10X | Removing extracted data at data/10X/pbmc4k/filtered_gene_bc_matrices\n",
      "[2020-07-17 21:12:42,427] INFO - scvi.dataset._anndata | Using data from adata.X\n",
      "[2020-07-17 21:12:42,428] INFO - scvi.dataset._anndata | Using batches from adata.obs[\"batch\"]\n",
      "[2020-07-17 21:12:42,430] INFO - scvi.dataset._anndata | Using labels from adata.obs[\"labels\"]\n",
      "[2020-07-17 21:12:42,431] INFO - scvi.dataset._anndata | Computing library size prior per batch\n",
      "[2020-07-17 21:12:42,456] INFO - scvi.dataset._anndata | Successfully registered anndata object containing 11990 cells, 3346 genes, and 2 batches \n",
      "Registered keys:['X', 'batch_indices', 'local_l_mean', 'local_l_var', 'labels']\n"
     ]
    }
   ],
   "source": [
    "pbmc_dataset = pbmc_dataset(save_path=os.path.join(save_path, \"10X/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-07-17 21:11:08,093] DEBUG - scvi.inference.autotune.all | All workers have died, check stdout/stderr for error tracebacks.\n",
      "[2020-07-17 21:11:08,095] DEBUG - scvi.inference.autotune.all | Worker watchdog finished, terminating workers and stopping listener.\n",
      "[2020-07-17 21:11:11,907] DEBUG - scvi.inference.autotune.all | fmin finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# best_trainer, trials = auto_tune_scvi_model(\n",
    "#     gene_dataset=pbmc_dataset,\n",
    "#     metric_name=\"entropy_batch_mixing\",\n",
    "#     data_loader_name=\"train_set\",\n",
    "#     parallel=True,\n",
    "#     exp_key=\"pbmc_entropy_batch_mixing\",\n",
    "#     train_func_specific_kwargs={\"n_epochs\": n_epochs},\n",
    "#     max_evals=max_evals,\n",
    "#     reserve_timeout=reserve_timeout,\n",
    "#     fmin_timeout=fmin_timeout,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we describe, using one of our Synthetic dataset, how to tune our annotation model `SCANVI` for, e.g, better accuracy on a 20% subset of the labelled data. Note that the model is trained in a semi-supervised framework, that is why we have a labelled and unlabelled dataset. Please, refer to the original [paper](https://www.biorxiv.org/content/10.1101/532895v1) for details on SCANVI!\n",
    "\n",
    "In this case, as described in our `annotation` notebook we may want to form the labelled/unlabelled sets using batch indices. Unfortunately, that requires a little \"by hand\" work. Even in that case, we are able to leverage the new autotune module to perform hyperparameter tuning. In order to do so, one has to write his own objective function and feed it to `auto_tune_scvi_model`.\n",
    "\n",
    "One can proceed as described below.\n",
    "Note three important conditions:\n",
    "\n",
    "* Since it is going to be pickled the objective should not be implemented in the \"__main__\" module, i.e an executable script or a notebook.\n",
    "* the objective should have the search space as its first attribute and a boolean `is_best_training` as its second.\n",
    "* If not using a cutstom search space, it should be expected to take the form of a dictionary with the following keys:\n",
    "\n",
    "    * `\"model_tunable_kwargs\"`\n",
    "    * `\"trainer_tunable_kwargs\"`\n",
    "    * `\"train_func_tunable_kwargs\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.utils.autotune_advanced_notebook import custom_objective_hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-07-17 21:15:51,567] INFO - scvi.dataset._utils | Downloading file at /Users/galen/scVI/tests/notebooks/data/simulation/simulation_1.loom\n",
      "[2020-07-17 21:16:01,421] WARNING - scvi.dataset._anndata | adata.X does not contain unnormalized count data. Are you sure this is what you want?\n",
      "[2020-07-17 21:16:01,421] INFO - scvi.dataset._anndata | Using data from adata.X\n",
      "[2020-07-17 21:16:01,422] INFO - scvi.dataset._anndata | Using batches from adata.obs[\"batch\"]\n",
      "[2020-07-17 21:16:01,425] INFO - scvi.dataset._anndata | Using labels from adata.obs[\"labels\"]\n",
      "[2020-07-17 21:16:01,426] INFO - scvi.dataset._anndata | Computing library size prior per batch\n",
      "[2020-07-17 21:16:01,817] INFO - scvi.dataset._anndata | Successfully registered anndata object containing 20000 cells, 2000 genes, and 2 batches \n",
      "Registered keys:['X', 'batch_indices', 'local_l_mean', 'local_l_var', 'labels']\n",
      "[2020-07-17 21:16:01,819] INFO - scvi.inference.autotune.all | Starting experiment: synthetic_dataset_scanvi\n",
      "[2020-07-17 21:16:01,819] DEBUG - scvi.inference.autotune.all | Using default parameter search space.\n",
      "[2020-07-17 21:16:01,822] INFO - scvi.inference.autotune.all | Using custom objective function.\n",
      "[2020-07-17 21:16:01,823] INFO - scvi.inference.autotune.all | Starting parallel hyperoptimization\n",
      "[2020-07-17 21:16:01,825] DEBUG - scvi.inference.autotune.all | Starting MongoDb process, logs redirected to ./mongo/mongo_logfile.txt.\n",
      "[2020-07-17 21:16:06,867] DEBUG - scvi.inference.autotune.all | Starting minimization procedure\n",
      "[2020-07-17 21:16:06,870] DEBUG - scvi.inference.autotune.all | Starting FminProcess.\n",
      "[2020-07-17 21:16:06,870] DEBUG - scvi.inference.autotune.all | Starting worker launcher\n",
      "[2020-07-17 21:16:06,873] DEBUG - scvi.inference.autotune.all | gpu_ids is None, defaulting to all 0 GPUs found by torch.\n",
      "[2020-07-17 21:16:07,100] DEBUG - scvi.inference.autotune.all | No GPUs found and n_cpu_wokers is None, defaulting to n_cpu_workers = 7 (os.cpu_count() - 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-07-17 21:16:07,115] DEBUG - scvi.inference.autotune.all | Listener listening...\n",
      "[2020-07-17 21:16:07,115] INFO - scvi.inference.autotune.all | Starting 1 worker.s for each of the 0 gpu.s set for use/found.\n",
      "[2020-07-17 21:16:07,119] INFO - scvi.inference.autotune.all | Starting 7 cpu worker.s\n",
      "[2020-07-17 21:16:16,677] DEBUG - scvi.inference.autotune.all | No timer, waiting for fmin...\n"
     ]
    }
   ],
   "source": [
    "synthetic_dataset = annotation_simulation(1, save_path=os.path.join(save_path, \"simulation/\"))\n",
    "objective_kwargs = dict(dataset=synthetic_dataset, n_epochs=n_epochs)\n",
    "best_trainer, trials = auto_tune_scvi_model(\n",
    "    custom_objective_hyperopt=custom_objective_hyperopt,\n",
    "    objective_kwargs=objective_kwargs,\n",
    "    parallel=True,\n",
    "    exp_key=\"synthetic_dataset_scanvi\",\n",
    "    max_evals=max_evals,\n",
    "    reserve_timeout=reserve_timeout,\n",
    "    fmin_timeout=fmin_timeout,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delayed populating, for very large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DISCLAIMER: We don't actually need this for the BrainLarge dataset with 720 genes, this is just an example.**\n",
    "\n",
    "The fact is that after building the objective function and feeding it to `hyperopt`, it is pickled on to the `MongoWorkers`. Thus, if you pass a loaded dataset as a partial argument to the objective function, and this dataset exceeds 4Gb, you'll get a `PickleError` (Objects larger than 4Gb can't be pickled).\n",
    "\n",
    "To remedy this issue, in case you have a very large dataset for which you want to perform hyperparameter optimization, you should subclass `scVI`'s `DownloadableDataset` or use one of its many existing subclasses, such that the dataset can be populated inside the objective function which is called by each worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brain_large_dataset_path = os.path.join(save_path, 'brainlarge_dataset_test.h5ad')\n",
    "\n",
    "# best_trainer, trials = auto_tune_scvi_model(\n",
    "#     gene_dataset=brain_large_dataset_path,\n",
    "#     parallel=True,\n",
    "#     exp_key=\"brain_large_dataset\",\n",
    "#     max_evals=max_evals,\n",
    "#     trainer_specific_kwargs={\n",
    "#         \"early_stopping_kwargs\": {\n",
    "#             \"early_stopping_metric\": \"elbo\",\n",
    "#             \"save_best_state_metric\": \"elbo\",\n",
    "#             \"patience\": 20,\n",
    "#             \"threshold\": 0,\n",
    "#             \"reduce_lr_on_plateau\": True,\n",
    "#             \"lr_patience\": 10,\n",
    "#             \"lr_factor\": 0.2,\n",
    "#         }\n",
    "#     },\n",
    "#     train_func_specific_kwargs={\"n_epochs\": n_epochs_brain_large},\n",
    "#     reserve_timeout=reserve_timeout,\n",
    "#     fmin_timeout=fmin_timeout,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with totalVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = scvi.data.pbmcs_10x_cite_seq(\n",
    "    save_path=save_path, run_setup_anndata=False\n",
    ")\n",
    "adata = if_not_test_else(adata, adata[:75, :50].copy())\n",
    "scvi.data.setup_anndata(\n",
    "    adata, batch_key=\"batch\", protein_expression_obsm_key=\"protein_expression\"\n",
    ")\n",
    "\n",
    "space = {\n",
    "    \"model_tunable_kwargs\": {\n",
    "        \"n_latent\": 5 + hp.randint(\"n_latent\", 11),  # [5, 15]\n",
    "        \"n_hidden\": hp.choice(\"n_hidden\", [64, 128, 256]),\n",
    "        \"n_layers_encoder\": 1 + hp.randint(\"n_layers\", 5),\n",
    "        \"dropout_rate_encoder\": hp.choice(\"dropout_rate\", [0.1, 0.3, 0.5, 0.7]),\n",
    "        \"gene_likelihood\": hp.choice(\"gene_likelihood\", [\"zinb\", \"nb\"]),\n",
    "    },\n",
    "    \"train_func_tunable_kwargs\": {\n",
    "        \"lr\": hp.choice(\"lr\", [0.01, 0.005, 0.001, 0.0005, 0.0001])\n",
    "    },\n",
    "}\n",
    "\n",
    "best_vae, trials = auto_tune_scvi_model(\n",
    "    gene_dataset=adata,\n",
    "    space=space,\n",
    "    parallel=True,\n",
    "    model_class=scvi.model.TOTALVI,\n",
    "    exp_key=\"totalvi_adata\",\n",
    "    train_func_specific_kwargs={\"n_epochs\": n_epochs},\n",
    "    max_evals=max_evals,\n",
    "    reserve_timeout=reserve_timeout,\n",
    "    fmin_timeout=fmin_timeout,\n",
    "    save_path=save_path,  # temp dir, see conftest.py\n",
    ")\n",
    "best_vae.get_latent_representation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def get_param_df(self):\n",
    "#         ddd = {}\n",
    "#         for i, trial in enumerate(self.trials):\n",
    "#             dd = {}\n",
    "#             dd[\"marginal_ll\"] = trial[\"result\"][\"loss\"]\n",
    "#             for item in trial[\"result\"][\"space\"].values():\n",
    "#                 for key, value in item.items():\n",
    "#                     dd[key] = value\n",
    "#             ddd[i] = dd\n",
    "#         df_space = pd.DataFrame(ddd)\n",
    "#         df_space = df_space.T\n",
    "#         n_params_dataset = np.vectorize(\n",
    "#             partial(\n",
    "#                 n_params, self.trainer.adata.uns[\"_scvi\"][\"summary_stats\"][\"n_vars\"]\n",
    "#             )\n",
    "#         )\n",
    "#         df_space[\"n_params\"] = n_params_dataset(\n",
    "#             df_space[\"n_layers\"], df_space[\"n_hidden\"], df_space[\"n_latent\"]\n",
    "#         )\n",
    "#         df_space = df_space[\n",
    "#             [\n",
    "#                 \"marginal_ll\",\n",
    "#                 \"n_layers\",\n",
    "#                 \"n_hidden\",\n",
    "#                 \"n_latent\",\n",
    "#                 \"reconstruction_loss\",\n",
    "#                 \"dropout_rate\",\n",
    "#                 \"lr\",\n",
    "#                 \"n_epochs\",\n",
    "#                 \"n_params\",\n",
    "#             ]\n",
    "#         ]\n",
    "#         df_space = df_space.sort_values(by=\"marginal_ll\")\n",
    "#         df_space[\"run index\"] = df_space.index\n",
    "#         df_space.index = np.arange(1, df_space.shape[0] + 1)\n",
    "#         return df_space\n",
    "\n",
    "\n",
    "# def n_params(n_vars, n_layers, n_hidden, n_latent):\n",
    "#     if n_layers == 0:\n",
    "#         res = 2 * n_vars * n_latent\n",
    "#     else:\n",
    "#         res = 2 * n_vars * n_hidden\n",
    "#         for i in range(n_layers - 1):\n",
    "#             res += 2 * n_hidden * n_hidden\n",
    "#         res += 2 * n_hidden * n_latent\n",
    "#     return res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}